# 1-1基础概念辨析

## 深度学习基础概念：Softmax 分类任务 (通俗版)

**核心逻辑**：**深度学习** = **正向“盲猜”** (考试) + **反向“找锅”** (订正)。

---

### 1. 场景设定 (The Setup)


假设我们要训练一个 AI 来区分 **猫、狗、猪** (3分类问题)。
为了简化计算，我们只看 **3个特征** (比如：毛发长度、耳朵形状、尾巴长度)。

* **输入 ($X$)**：3个特征的数据。
* **参数 ($W$)**：一个 $3 \times 3$ 的矩阵 (连接 3个特征 到 3个类别)。

---

### 2. 第一阶段：正向传播 (Forward Propagation)
> **直觉理解**：**“盲猜” / “考试”**
> 数据从左到右流过网络，AI 此时还不知道答案，只是根据目前的认知强行给出一个结果。

#### 2.1 线性变换 —— “打分”
AI 根据权重矩阵 $W$，给每个类别打一个原始分数 (Logits)。

* **公式**：
  $$Z = W \cdot X + b$$
* **直觉**：
  * $W$ (权重) 代表“特征”对“类别”的重要性。
  * 算出来的 $Z$ (比如 `[2.0, 1.0, 0.1]`) 是**生肉**，有正有负，还不是概率。

#### 2.2 Softmax —— “变成概率”
把上面那个随意的分数，强行压缩到 0~1 之间，并且加起来等于 1。

* **公式**：
  $$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}}$$
* **直觉**：
  * “归一化”。
  * $Z$ 变成了 $A$ (比如 `[0.7, 0.2, 0.1]`)。
  * 此时 AI 敢说了：“我有 70% 的把握它是猫”。

#### 2.3 计算 Loss —— “对答案”
拿着 AI 的预测结果，去跟标准答案 (Label) 对比，看错得离谱不离谱。

* **公式 (交叉熵)**：
  $$Loss = - \sum y_{true} \cdot \log(A)$$
* **直觉**：
  * 如果 Loss 很大，说明 AI 只有 10% 的把握是猫，但照片真的是猫（猜错了）。
  * 如果 Loss 接近 0，说明 AI 猜对了。

---

### 3. 第二阶段：反向传播 (Backward Propagation)
> **直觉理解**：**“找锅” / “复盘”**
> 发现 Loss 不为 0 (考砸了)，我们要倒推回去，看看是哪一个权重导致了错误，把它揪出来修改。

#### 3.1 链式法则 (Chain Rule) —— “剥洋葱”
因为计算过程是一层包一层的 ($W \to Z \to A \to Loss$)，我们需要一层层求导。

* **公式**：
  $$\frac{\partial Loss}{\partial W} = \underbrace{\frac{\partial Loss}{\partial A}}_{\text{错在哪？}} \times \underbrace{\frac{\partial A}{\partial Z}}_{\text{Softmax导数}} \times \underbrace{\frac{\partial Z}{\partial W}}_{\text{输入是啥？}}$$
  
* **直觉**：
  * 这就是在算 **梯度 (Gradient)**。
  * 梯度就是“锅”的大小。如果某个 $w$ 的梯度很大，说明它对这次错误负主要责任。

#### 3.2 参数更新 (Update) —— “改错”
找到了“锅” (梯度 $G$)，我们就要修改原来的参数 $W$，让它下次别犯同样的错。

* **公式**：
  $$W_{new} = W_{old} - \eta \cdot G$$
* **直觉**：
  * **$\eta$ (学习率)**：步子迈多大。
  * **减号 (-)**：往梯度的**反方向**走（梯度是让 Loss 变大的方向，我们要让 Loss 变小）。

---

### 4. 为什么作业要用 3x3 矩阵？
* **1x1 太简单**：看不出“行”与“列”相乘的规律。
* **784x10 太复杂**：(MNIST 的维度) 手算会算死人。
* **3x3 刚刚好**：
  * 既能展示矩阵运算的全过程 (Matrix Multiplication)。
  * 又在人类手算的极限范围内。
  * 麻雀虽小，五脏俱全。

---

## 总结口诀
1. **Forward**:  $X$ 乘 $W$ 变 $Z$，Softmax 变概率，Loss 看差距。
2. **Backward**: 差距回传找梯度，链式法则剥洋葱。
3. **Update**:  权重减去学习率乘梯度，明天考试能满分。


## 深度学习手推笔记：Softmax 回归与反向传播

**日期**: 2026-02-09
**主题**: 手算 Softmax 正向与反向传播全流程
**标签**: #DeepLearning #Math #Softmax #Backprop

---

### 1. 实验设置 (Setup)

为了演示神经网络的训练过程，我们设定一个 3 分类问题。
**学习率 (Learning Rate)**: $\eta = 0.1$

### 1.1 输入数据 (Input Sample)
$$
X = \begin{bmatrix} 0.5 \\ 0.6 \\ 0.7 \end{bmatrix}
$$

### 1.2 初始参数 (Initial Parameters)
**权重矩阵 $W$** (3x3):
$$
W = \begin{bmatrix}
0.1 & 0.2 & 0.3 \\
0.4 & 0.5 & 0.6 \\
0.7 & 0.8 & 0.9
\end{bmatrix}
$$

**偏置向量 $b$** (3x1):
$$
b = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.3 \end{bmatrix}
$$

### 1.3 真实标签 (True Label)
$$
Y_{true} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}
$$
> **注**: $Y_{true}$ 表示这张图片是**第 2 类**（索引为1）。

---

### 2. 第一阶段：正向传播 (Forward Propagation)
> **目的**: 让模型基于当前参数进行一次“盲猜”。

### 2.1 线性计算 (Linear Score)
公式：$Z = W \cdot X + b$

逐行计算得分 ($z_i$):
* **$z_1$ (第1类)**: $(0.1 \times 0.5 + 0.2 \times 0.6 + 0.3 \times 0.7) + 0.1 = \mathbf{0.48}$
* **$z_2$ (第2类)**: $(0.4 \times 0.5 + 0.5 \times 0.6 + 0.6 \times 0.7) + 0.2 = \mathbf{1.12}$
* **$z_3$ (第3类)**: $(0.7 \times 0.5 + 0.8 \times 0.6 + 0.9 \times 0.7) + 0.3 = \mathbf{1.76}$

得到的 Logits 向量：
$$
Z = \begin{bmatrix} 0.48 \\ 1.12 \\ 1.76 \end{bmatrix}
$$

### 2.2 Softmax 激活 (Activation)
> **目的**: 将得分转换为概率分布。

公式：$a_i = \frac{e^{z_i}}{\sum e^{z_j}}$

1.  **求指数**:
    * $e^{0.48} \approx 1.616$
    * $e^{1.12} \approx 3.065$
    * $e^{1.76} \approx 5.812$
2.  **求和 (分母)**: $1.616 + 3.065 + 5.812 = \mathbf{10.493}$
3.  **求概率**:
    * $a_1 = 1.616 / 10.493 \approx \mathbf{0.154}$ (15.4%)
    * $a_2 = 3.065 / 10.493 \approx \mathbf{0.292}$ (29.2%)
    * $a_3 = 5.812 / 10.493 \approx \mathbf{0.554}$ (55.4%)

**当前预测结果**: $A = [0.154, 0.292, 0.554]^T$
> **分析**: 模型认为第 3 类概率最大 (55.4%)，但真实标签是第 2 类。**模型猜错了**。

---

### 3. 第二阶段：反向传播 (Backward Propagation)
> **目的**: 计算梯度，找出“是谁导致了错误”。

### 3.1 计算误差项 ($\delta$)
对于 Softmax + CrossEntropy Loss，误差项推导结果非常简洁：
$$
\delta = \text{预测值} - \text{真实值} = A - Y_{true}
$$

$$
\delta = \begin{bmatrix} 0.154 \\ 0.292 \\ 0.554 \end{bmatrix} - \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \mathbf{0.154} \\ \mathbf{-0.708} \\ \mathbf{0.554} \end{bmatrix}
$$

> **直觉解释**:
> * 正数 (0.154, 0.554): 预测偏高了，需要降低。
> * 负数 (-0.708): 预测偏低了 (正确答案)，需要提高。

### 3.2 计算梯度 (Gradients)

**1. 偏置的梯度 ($db$)**:
$$
db = \delta = \begin{bmatrix} 0.154 \\ -0.708 \\ 0.554 \end{bmatrix}
$$

**2. 权重的梯度 ($dW$)**:
公式：$dW = \delta \cdot X^T$ (误差向量 $\times$ 输入向量的转置)

$$
dW = \begin{bmatrix} 0.154 \\ -0.708 \\ 0.554 \end{bmatrix} \times \begin{bmatrix} 0.5 & 0.6 & 0.7 \end{bmatrix}
$$

逐行计算：
* **第1行 (针对第1类)**: $0.154 \times [0.5, 0.6, 0.7] \approx [\mathbf{0.077}, \mathbf{0.092}, \mathbf{0.108}]$
* **第2行 (针对第2类)**: $-0.708 \times [0.5, 0.6, 0.7] \approx [\mathbf{-0.354}, \mathbf{-0.425}, \mathbf{-0.496}]$
* **第3行 (针对第3类)**: $0.554 \times [0.5, 0.6, 0.7] \approx [\mathbf{0.277}, \mathbf{0.332}, \mathbf{0.388}]$

---

### 4. 第三阶段：参数更新 (Parameter Update)
> **目的**: 修改参数，让模型下次能猜对。

公式：$Param_{new} = Param_{old} - \eta \times Gradient$
(设定学习率 $\eta = 0.1$)

### 4.1 更新偏置 ($b$)
以第 2 类（正确答案）为例：
$$
b_2^{new} = 0.2 - (0.1 \times -0.708) = 0.2 + 0.0708 = \mathbf{0.2708}
$$
> **结果**: $b_2$ 变大了，下次正向传播时 $z_2$ 得分会更高。

### 4.2 更新权重 ($W$)
以第 2 行权重（负责预测正确答案的那一行）为例：
* $w_{2,1}^{new} = 0.4 - (0.1 \times -0.354) = \mathbf{0.4354}$
* $w_{2,2}^{new} = 0.5 - (0.1 \times -0.425) = \mathbf{0.5425}$
* $w_{2,3}^{new} = 0.6 - (0.1 \times -0.496) = \mathbf{0.6496}$

> **总结**: 因为预测错了，反向传播机制强行**增加**了正确类别对应的权重和偏置，同时**减小**了错误类别（第1、3类）的权重。

---

## 5. 补充知识：关于初始化与归一化

### Q: 为什么我在手算时可以随意设置 W 和 X？
在手算练习中，我们设置 $W$ (0.1~0.9) 和 $X$ (0.5~0.7) 主要是为了：
1.  **方便追踪计算**：有规律的数字容易看出计算路径。
2.  **避免数值溢出**：数字都在 0~1 之间，$e^z$ 不会爆炸。

### Q: 真实工程中 (PyTorch) 是怎么做的？
在实际 AI 训练中，不能这样随意设置，否则模型学不会。

#### 1. 权重初始化 (Weight Initialization)
* **禁忌**: 不能全部设为 0，也不能设为同样的数（会导致**对称性陷阱**，所有神经元学到一样的东西）。
* **正解**: 使用 **随机初始化** (如 Xavier 或 Kaiming 初始化)。
    * 例如：`nn.Linear` 会生成类似 `[-0.012, 0.045, ...]` 的随机小数。
    * **目的**: 打破对称性，让每个神经元各司其职。

#### 2. 输入归一化 (Input Normalization)
* **禁忌**: 直接把像素值 (0~255) 喂给网络。
    * $255 \times W \to$ 巨大的 $Z \to$ $e^{1000}$ (溢出/梯度消失)。
* **正解**: 将输入数据除以 255，或标准化到 `[0, 1]` 或 `[-1, 1]` 区间。
    * **目的**: 保证数值稳定性，加快收敛速度。