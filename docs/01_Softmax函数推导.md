# 1-1基础概念辨析
## 概念辨析：
## 深度学习基础概念：Softmax 分类任务 (通俗版)

**核心逻辑**：**深度学习** = **正向“盲猜”** (考试) + **反向“找锅”** (订正)。

---

## 1. 场景设定 (The Setup)
假设我们要训练一个 AI 来区分 **猫、狗、猪** (3分类问题)。
为了简化计算，我们只看 **3个特征** (比如：毛发长度、耳朵形状、尾巴长度)。

* **输入 ($X$)**：3个特征的数据。
* **参数 ($W$)**：一个 $3 \times 3$ 的矩阵 (连接 3个特征 到 3个类别)。

---

## 2. 第一阶段：正向传播 (Forward Propagation)
> **直觉理解**：**“盲猜” / “考试”**
> 数据从左到右流过网络，AI 此时还不知道答案，只是根据目前的认知强行给出一个结果。

### 2.1 线性变换 —— “打分”
AI 根据权重矩阵 $W$，给每个类别打一个原始分数 (Logits)。

* **公式**：
  $$Z = W \cdot X + b$$
* **直觉**：
  * $W$ (权重) 代表“特征”对“类别”的重要性。
  * 算出来的 $Z$ (比如 `[2.0, 1.0, 0.1]`) 是**生肉**，有正有负，还不是概率。

### 2.2 Softmax —— “变成概率”
把上面那个随意的分数，强行压缩到 0~1 之间，并且加起来等于 1。

* **公式**：
  $$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum e^{z_j}}$$
* **直觉**：
  * “归一化”。
  * $Z$ 变成了 $A$ (比如 `[0.7, 0.2, 0.1]`)。
  * 此时 AI 敢说了：“我有 70% 的把握它是猫”。

### 2.3 计算 Loss —— “对答案”
拿着 AI 的预测结果，去跟标准答案 (Label) 对比，看错得离谱不离谱。

* **公式 (交叉熵)**：
  $$Loss = - \sum y_{true} \cdot \log(A)$$
* **直觉**：
  * 如果 Loss 很大，说明 AI 只有 10% 的把握是猫，但照片真的是猫（猜错了）。
  * 如果 Loss 接近 0，说明 AI 猜对了。

---

## 3. 第二阶段：反向传播 (Backward Propagation)
> **直觉理解**：**“找锅” / “复盘”**
> 发现 Loss 不为 0 (考砸了)，我们要倒推回去，看看是哪一个权重导致了错误，把它揪出来修改。

### 3.1 链式法则 (Chain Rule) —— “剥洋葱”
因为计算过程是一层包一层的 ($W \to Z \to A \to Loss$)，我们需要一层层求导。

* **公式**：
  $$\frac{\partial Loss}{\partial W} = \underbrace{\frac{\partial Loss}{\partial A}}_{\text{错在哪？}} \times \underbrace{\frac{\partial A}{\partial Z}}_{\text{Softmax导数}} \times \underbrace{\frac{\partial Z}{\partial W}}_{\text{输入是啥？}}$$
  
* **直觉**：
  * 这就是在算 **梯度 (Gradient)**。
  * 梯度就是“锅”的大小。如果某个 $w$ 的梯度很大，说明它对这次错误负主要责任。

### 3.2 参数更新 (Update) —— “改错”
找到了“锅” (梯度 $G$)，我们就要修改原来的参数 $W$，让它下次别犯同样的错。

* **公式**：
  $$W_{new} = W_{old} - \eta \cdot G$$
* **直觉**：
  * **$\eta$ (学习率)**：步子迈多大。
  * **减号 (-)**：往梯度的**反方向**走（梯度是让 Loss 变大的方向，我们要让 Loss 变小）。

---

## 4. 为什么作业要用 3x3 矩阵？
* **1x1 太简单**：看不出“行”与“列”相乘的规律。
* **784x10 太复杂**：(MNIST 的维度) 手算会算死人。
* **3x3 刚刚好**：
  * 既能展示矩阵运算的全过程 (Matrix Multiplication)。
  * 又在人类手算的极限范围内。
  * 麻雀虽小，五脏俱全。

---

### 总结口诀
1. **Forward**:  $X$ 乘 $W$ 变 $Z$，Softmax 变概率，Loss 看差距。
2. **Backward**: 差距回传找梯度，链式法则剥洋葱。
3. **Update**:  权重减去学习率乘梯度，明天考试能满分。